{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e4c51d-f7bf-43bc-b2ff-0f5367ff5e19",
   "metadata": {},
   "source": [
    "## üå± Stemming in NLP ‚Äî Detailed Explanation\n",
    "\n",
    "Stemming is a **text normalization technique** used in **Natural Language Processing (NLP)** to reduce words to their **root or base form**, called a *stem*. The main goal is to treat related words as the same word by removing suffixes.\n",
    "\n",
    "For example:\n",
    "- running ‚Üí run  \n",
    "- played ‚Üí play  \n",
    "- studies ‚Üí studi  \n",
    "\n",
    "Note: The stemmed word **may not always be a valid dictionary word**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Do We Need Stemming?\n",
    "\n",
    "In text data:\n",
    "- The same word can appear in different forms  \n",
    "- This increases vocabulary size unnecessarily  \n",
    "\n",
    "Stemming helps by:\n",
    "- Reducing vocabulary size  \n",
    "- Improving model efficiency  \n",
    "- Grouping similar words together  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ How Stemming Works (Intuition)\n",
    "\n",
    "> Remove common suffixes to get the root meaning of a word.\n",
    "\n",
    "Stemming applies **rule-based heuristics** to strip endings like:\n",
    "- -ing  \n",
    "- -ed  \n",
    "- -ly  \n",
    "- -s  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Common Stemming Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Porter Stemmer\n",
    "- Most widely used  \n",
    "- Applies a series of rules  \n",
    "- Fast and efficient  \n",
    "\n",
    "Example:\n",
    "- connectivity ‚Üí connect  \n",
    "- relational ‚Üí relat  \n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Snowball Stemmer\n",
    "- Improved version of Porter  \n",
    "- Supports multiple languages  \n",
    "- More aggressive than Porter  \n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Lancaster Stemmer\n",
    "- Very aggressive  \n",
    "- Can over-stem words  \n",
    "\n",
    "Example:\n",
    "- maximum ‚Üí max  \n",
    "- university ‚Üí univers  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Aim of Stemming\n",
    "\n",
    "The main aims are to:\n",
    "- Normalize text  \n",
    "- Reduce dimensionality  \n",
    "- Improve performance in NLP models  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Why We Use Stemming\n",
    "\n",
    "Stemming is used because it:\n",
    "- Speeds up text processing  \n",
    "- Reduces memory usage  \n",
    "- Improves results in tasks like search engines and text classification  \n",
    "\n",
    "---\n",
    "\n",
    "### üìä Advantages\n",
    "\n",
    "- Simple and fast  \n",
    "- Reduces vocabulary size  \n",
    "- Easy to implement  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "- May produce non-meaningful words  \n",
    "- Can over-stem or under-stem  \n",
    "- Loses grammatical correctness  \n",
    "\n",
    "---\n",
    "\n",
    "### üß† Stemming vs Lemmatization (Quick)\n",
    "\n",
    "| Feature | Stemming | Lemmatization |\n",
    "|-------|---------|---------------|\n",
    "| Output | Root form | Dictionary word |\n",
    "| Accuracy | Lower | Higher |\n",
    "| Speed | Faster | Slower |\n",
    "| Uses grammar | No | Yes |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† In Simple Words\n",
    "\n",
    "Stemming cuts words down to their base form by removing suffixes. It is fast and useful but may produce words that are not real dictionary terms.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Summary\n",
    "\n",
    "Stemming is a lightweight NLP technique that reduces words to their stems to simplify text processing. It is commonly used when speed is more important than linguistic accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458f634-ffb7-428c-8a96-8b4aa5026ce9",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38dfed-fe21-4f6a-813d-18e62615c983",
   "metadata": {},
   "source": [
    "## üçÉ Lemmatization in NLP ‚Äî Detailed Explanation\n",
    "\n",
    "Lemmatization is a **text normalization technique** in **Natural Language Processing (NLP)** that reduces words to their **base or dictionary form**, called a *lemma*. Unlike stemming, lemmatization considers the **meaning and grammatical context** of a word.\n",
    "\n",
    "For example:\n",
    "- running ‚Üí run  \n",
    "- better ‚Üí good  \n",
    "- studies ‚Üí study  \n",
    "\n",
    "The output of lemmatization is always a **valid dictionary word**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Do We Need Lemmatization?\n",
    "\n",
    "Text data often contains:\n",
    "- Different forms of the same word  \n",
    "- Grammatical variations (tense, number, degree)  \n",
    "\n",
    "Lemmatization helps by:\n",
    "- Reducing vocabulary size  \n",
    "- Preserving actual meaning  \n",
    "- Improving NLP model accuracy  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ How Lemmatization Works (Intuition)\n",
    "\n",
    "> Understand the word‚Äôs part of speech and return its correct base form.\n",
    "\n",
    "Lemmatization uses:\n",
    "- Vocabulary lookup  \n",
    "- Morphological analysis  \n",
    "- Part-of-Speech (POS) tagging  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Examples of Lemmatization\n",
    "\n",
    "| Word | Lemma |\n",
    "|----|------|\n",
    "| running | run |\n",
    "| better | good |\n",
    "| mice | mouse |\n",
    "| was | be |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Common Lemmatization Tools\n",
    "\n",
    "- **WordNet Lemmatizer (NLTK)**  \n",
    "- **spaCy Lemmatizer**\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Aim of Lemmatization\n",
    "\n",
    "The main aims are to:\n",
    "- Normalize text  \n",
    "- Preserve semantic meaning  \n",
    "- Improve language understanding  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Why We Use Lemmatization\n",
    "\n",
    "Lemmatization is used because it:\n",
    "- Produces meaningful words  \n",
    "- Improves accuracy in NLP tasks  \n",
    "- Handles grammatical variations properly  \n",
    "\n",
    "---\n",
    "\n",
    "### üìä Advantages\n",
    "\n",
    "- Outputs real dictionary words  \n",
    "- Preserves context and meaning  \n",
    "- More accurate than stemming  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "- Slower than stemming  \n",
    "- Requires POS tagging  \n",
    "- More computationally expensive  \n",
    "\n",
    "---\n",
    "\n",
    "### üß† Lemmatization vs Stemming (Quick)\n",
    "\n",
    "| Feature | Lemmatization | Stemming |\n",
    "|-------|---------------|----------|\n",
    "| Output | Dictionary word | Root form |\n",
    "| Accuracy | High | Lower |\n",
    "| Speed | Slower | Faster |\n",
    "| Grammar awareness | Yes | No |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† In Simple Words\n",
    "\n",
    "Lemmatization converts words into their meaningful base form by understanding grammar and context, making text cleaner and more accurate for NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Summary\n",
    "\n",
    "Lemmatization is a powerful NLP preprocessing technique that reduces words to their correct base form while preserving meaning. It is preferred when accuracy and language understanding matter more than speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8eed2-cb0b-48c8-9d88-3a3dd76b462f",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb292161-4a19-4299-a63e-c2116c363e15",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Tokenization in NLP\n",
    "\n",
    "Tokenization is a **text preprocessing technique** in **Natural Language Processing (NLP)** where raw text is broken into smaller units called **tokens**. Tokens are the basic elements that NLP models use to understand and process text.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Tokenization is Important\n",
    "\n",
    "- Machine learning models cannot process raw text directly  \n",
    "- Tokenization converts text into manageable units  \n",
    "- It is the **first and most essential step** in any NLP pipeline  \n",
    "- Required for techniques like BoW, TF-IDF, and Word Embeddings  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Types of Tokenization\n",
    "\n",
    "#### 1Ô∏è‚É£ Word Tokenization  \n",
    "Splits text into individual words.\n",
    "\n",
    "Example:  \n",
    "\"I love machine learning\"  \n",
    "‚Üí [\"I\", \"love\", \"machine\", \"learning\"]\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Sentence Tokenization  \n",
    "Splits text into sentences.\n",
    "\n",
    "Example:  \n",
    "\"I love ML. It is powerful.\"  \n",
    "‚Üí [\"I love ML.\", \"It is powerful.\"]\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Subword Tokenization (Very Important)  \n",
    "Breaks words into smaller meaningful units.\n",
    "\n",
    "Example:  \n",
    "\"unhappiness\" ‚Üí [\"un\", \"happi\", \"ness\"]\n",
    "\n",
    "Used in modern NLP models like:\n",
    "- BERT (WordPiece)  \n",
    "- GPT (BPE)  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Character Tokenization  \n",
    "Splits text into individual characters.\n",
    "\n",
    "Example:  \n",
    "\"cat\" ‚Üí [\"c\", \"a\", \"t\"]\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Tokenization in Modern NLP Models\n",
    "\n",
    "| Model | Tokenization Method |\n",
    "|------|--------------------|\n",
    "| BERT | WordPiece |\n",
    "| GPT | Byte Pair Encoding (BPE) |\n",
    "| RoBERTa | BPE |\n",
    "| ALBERT | SentencePiece |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Aim of Tokenization\n",
    "\n",
    "- Convert text into tokens  \n",
    "- Enable numerical representation  \n",
    "- Improve NLP model understanding  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Challenges in Tokenization\n",
    "\n",
    "- Handling punctuation and emojis  \n",
    "- Multiple languages  \n",
    "- Contractions and slang  \n",
    "- Unknown (OOV) words  \n",
    "\n",
    "---\n",
    "\n",
    "### üß† In Simple Words\n",
    "\n",
    "Tokenization cuts text into small pieces so that machines can understand and analyze language.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Summary\n",
    "\n",
    "Tokenization is the foundation of NLP. From basic word splitting to advanced subword techniques, it enables effective text processing and modern language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a4512-7f82-4697-8596-3a89fdd73e08",
   "metadata": {},
   "source": [
    "STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16399887-769b-410a-8112-73b11a75d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7116835-6fbc-43f4-8f52-a3c84ab4e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a2fc88-649b-4cbd-adb4-a08f426cf2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255be671-b631-4f47-851b-bbf8049c3bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eabdcb8-574d-4887-b583-6c16877e2b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('finally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0913c1ce-4478-4e8c-a151-f145780cf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating','eats','eater','eaten','writing','writes','programming','programmer','programes','history','geography','finalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920f4ed5-b589-4003-bb2d-27c561abeed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eats ----> eat\n",
      "eater ----> eater\n",
      "eaten ----> eaten\n",
      "writing ----> write\n",
      "writes ----> write\n",
      "programming ----> program\n",
      "programmer ----> programm\n",
      "programes ----> program\n",
      "history ----> histori\n",
      "geography ----> geographi\n",
      "finalized ----> final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word,'---->',stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5c6c4-2ccc-420e-a752-9d5a81338bd9",
   "metadata": {},
   "source": [
    "SNOWBALL STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb070960-6def-49e8-9bff-ed16905fe482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6e54c6-9143-4b04-9bd0-081a9cacbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cefd6be5-fe08-499b-b907-7789ab35c6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x25299eb5880>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b575dba-ce77-4019-b065-aa9666bea409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'program'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow.stem('programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afbc3141-4fd5-4d79-a313-7087cf011eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ------> eat\n",
      "eats ------> eat\n",
      "eater ------> eater\n",
      "eaten ------> eaten\n",
      "writing ------> write\n",
      "writes ------> write\n",
      "programming ------> program\n",
      "programmer ------> programm\n",
      "programes ------> program\n",
      "history ------> histori\n",
      "geography ------> geographi\n",
      "finalized ------> final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word,'------>',snow.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e1f00-bc18-4da3-90ce-fb6a3d01a36e",
   "metadata": {},
   "source": [
    "REGEX STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e7d9324-853f-4fc4-8659-d17bc3f24029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc94593c-9193-4d89-8cb5-4f9fb31c2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RegexpStemmer('ing$',min=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "899caaa8-62a9-46ad-826d-b90c27a36bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ring'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.stem('ringing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a4be433-1212-4f84-8903-b1d3158fe5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runn'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76c5e0ce-cfe6-40fd-b53d-51c28b52951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['riniging','working','coaching','running']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fecda85a-6640-4f6f-88b8-22e7a3f03af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riniging ------> rinig\n",
      "working ------> work\n",
      "coaching ------> coach\n",
      "running ------> runn\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    print(sent,'------>',reg.stem(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019683e-7605-4761-8753-70cb6f0f0392",
   "metadata": {},
   "source": [
    "LEMMITIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97cf6c3d-eda6-4602-bfc5-64cc655c2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ee4e7ac-e4da-4ca5-8c35-493c2abb8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e0048dc-7302-40fe-90dc-a995ae7fb676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swim'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('swimming',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9e785f5-1c91-4339-8c88-e6c9d28b0a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPOS noun=n,\\nverb=v,\\nadverb=r,\\nadjactive=a\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "POS noun=n,\n",
    "verb=v,\n",
    "adverb=r,\n",
    "adjactive=a\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f729ff3f-819f-46ef-b06f-d6291b62b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ------> eat\n",
      "eats ------> eats\n",
      "eater ------> eater\n",
      "eaten ------> eaten\n",
      "writing ------> writ\n",
      "writes ------> writes\n",
      "programming ------> programm\n",
      "programmer ------> programmer\n",
      "programes ------> programes\n",
      "history ------> history\n",
      "geography ------> geography\n",
      "finalized ------> finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word,'------>',reg.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760fcd9f-9f98-4c56-8412-4836e7ecc1bd",
   "metadata": {},
   "source": [
    "BAG OF WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e0f02-78f0-49ad-815f-cb0edb4c1ea8",
   "metadata": {},
   "source": [
    "## üëú Bag of Words (BoW) ‚Äî Detailed Explanation\n",
    "\n",
    "Bag of Words (BoW) is a **text representation technique** used in **Natural Language Processing (NLP)**. It converts text data into **numerical feature vectors** by counting how many times each word appears in a document.\n",
    "\n",
    "BoW ignores grammar and word order and focuses only on **word frequency**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Core Idea of Bag of Words (Simple Intuition)\n",
    "\n",
    "> A document is represented by the words it contains and their counts.\n",
    "\n",
    "The text is treated as a ‚Äúbag‚Äù of words where only presence or frequency matters.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ How Bag of Words Works (Step-by-Step)\n",
    "\n",
    "1. Collect all documents  \n",
    "2. Create a vocabulary of unique words  \n",
    "3. Count the frequency of each word in every document  \n",
    "4. Represent each document as a vector of word counts  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Example\n",
    "\n",
    "Documents:\n",
    "- Doc1: \"I love machine learning\"  \n",
    "- Doc2: \"I love data science\"  \n",
    "\n",
    "Vocabulary:\n",
    "`[I, love, machine, learning, data, science]`\n",
    "\n",
    "BoW Representation:\n",
    "\n",
    "| Document | I | love | machine | learning | data | science |\n",
    "|--------|---|------|---------|----------|------|---------|\n",
    "| Doc1 | 1 | 1 | 1 | 1 | 0 | 0 |\n",
    "| Doc2 | 1 | 1 | 0 | 0 | 1 | 1 |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Aim of Bag of Words\n",
    "\n",
    "The main aims are to:\n",
    "- Convert text into numbers  \n",
    "- Capture word importance  \n",
    "- Enable machine learning models to process text  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Why We Use Bag of Words\n",
    "\n",
    "Bag of Words is used because it:\n",
    "- Is simple and easy to implement  \n",
    "- Works well for text classification  \n",
    "- Requires minimal preprocessing  \n",
    "- Serves as a baseline NLP technique  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Variants of Bag of Words\n",
    "\n",
    "- **Binary BoW** (word presence: 0/1)  \n",
    "- **Count BoW** (word frequency)  \n",
    "- **N-Grams** (captures short word sequences)  \n",
    "\n",
    "---\n",
    "\n",
    "### üìä Advantages\n",
    "\n",
    "- Simple and fast  \n",
    "- Easy to interpret  \n",
    "- Works well with classical ML models  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "- Ignores word order  \n",
    "- Cannot capture context or meaning  \n",
    "- Large vocabulary leads to high dimensionality  \n",
    "- Sensitive to common words  \n",
    "\n",
    "---\n",
    "\n",
    "### üß† Bag of Words vs TF-IDF (Quick)\n",
    "\n",
    "| Feature | Bag of Words | TF-IDF |\n",
    "|-------|-------------|--------|\n",
    "| Weighting | Raw counts | Importance-based |\n",
    "| Common words | Overweighted | Downweighted |\n",
    "| Complexity | Simple | Moderate |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† In Simple Words\n",
    "\n",
    "Bag of Words converts text into numbers by counting words. It is easy to use but does not understand meaning or context.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Summary\n",
    "\n",
    "Bag of Words is a foundational NLP technique that transforms text into numerical vectors using word frequencies. Despite its simplicity, it is widely used for text classification and baseline NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a6b2447-0d7f-4736-976f-2bbc3b583067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ec72825-a5d4-42e8-9d1d-4232c062439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['i love data science data','data science is amazing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65489a8a-cc31-4af3-84ce-6230ecbd573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cdac8ea-9455-49b7-80bb-bc36c1daeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform(corpus)\n",
    "bow_array = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e6e5b30-e584-4f74-ba6a-3b071998ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing' 'data' 'is' 'love' 'science']\n",
      "Bow Matrix: [[0 2 0 1 1]\n",
      " [1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\",vectorizer.get_feature_names_out())\n",
    "print('Bow Matrix:',bow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf6d242-c93a-48b1-8a6f-8f565d3145af",
   "metadata": {},
   "source": [
    "CO-OCCURENCE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb403111-7830-407f-86a5-f85a8dd304e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c248a78-c75a-40ad-86aa-a102d57efea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"\"\"data science is amazing \n",
    "         machine learning is a part of data science\n",
    "         deep learning and machine learning is very useful\n",
    "         data science and deep learning go hand in hand\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1676430c-8d67-4cda-a6b8-7e5293bddd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.lower().split()\n",
    "vocabulary = sorted(list(set(words)))\n",
    "word_to_index = {word: i for i,word in enumerate(vocabulary)}\n",
    "window_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "393ccaf8-96c2-400f-8ddc-634b12413c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'data',\n",
       " 'deep',\n",
       " 'go',\n",
       " 'hand',\n",
       " 'in',\n",
       " 'is',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'of',\n",
       " 'part',\n",
       " 'science',\n",
       " 'useful',\n",
       " 'very']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2583def0-79cb-475e-88f9-aeda91c461d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'amazing': 1,\n",
       " 'and': 2,\n",
       " 'data': 3,\n",
       " 'deep': 4,\n",
       " 'go': 5,\n",
       " 'hand': 6,\n",
       " 'in': 7,\n",
       " 'is': 8,\n",
       " 'learning': 9,\n",
       " 'machine': 10,\n",
       " 'of': 11,\n",
       " 'part': 12,\n",
       " 'science': 13,\n",
       " 'useful': 14,\n",
       " 'very': 15}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c3b9180-3434-4dca-a6cf-bf9533856d74",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2702319611.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfor j in range(max =0,i - window_size) ,min(len(words),i + window_size + 1 ):\u001b[39m\n                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "matrix = np.zeroes(len(vocabulary),len(vocabulary),dtype=int)\n",
    "\n",
    "for i in range(len(words)):\n",
    "    current_words = words[i]\n",
    "    for j in range(max =0,i - window_size) ,min(len(words),i + window_size + 1 ):\n",
    "        for !=j:\n",
    "        neighbour_words = words[j]\n",
    "        matrix[word_to_index[current_word],word_to_index[neighbour_word]] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "183e7826-5b9f-450d-bddb-9307ce295896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create co-occurrence matrix\n",
    "matrix = np.zeros((len(vocabulary), len(vocabulary)), dtype=int)\n",
    "\n",
    "for i in range(len(words)):\n",
    "    current_word = words[i]\n",
    "\n",
    "    start = max(0, i - window_size)\n",
    "    end = min(len(words), i + window_size + 1)\n",
    "\n",
    "    for j in range(start, end):\n",
    "        if i != j:\n",
    "            neighbour_word = words[j]\n",
    "            matrix[\n",
    "                word_to_index[current_word],\n",
    "                word_to_index[neighbour_word]\n",
    "            ] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "96ae5b12-fbde-44fa-9794-047f84c391a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(matrix,index=vocabulary,columns=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25a0f856-603a-4d5a-9df2-d58140ed305e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>data</th>\n",
       "      <th>deep</th>\n",
       "      <th>go</th>\n",
       "      <th>hand</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>part</th>\n",
       "      <th>science</th>\n",
       "      <th>useful</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hand</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>part</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a  amazing  and  data  deep  go  hand  in  is  learning  machine  \\\n",
       "a         0        0    0     0     0   0     0   0   1         0        0   \n",
       "amazing   0        0    0     0     0   0     0   0   1         0        1   \n",
       "and       0        0    0     0     1   0     0   0   0         1        1   \n",
       "data      0        0    0     0     0   0     0   0   0         0        0   \n",
       "deep      0        0    1     0     0   0     0   0   0         2        0   \n",
       "go        0        0    0     0     0   0     1   0   0         1        0   \n",
       "hand      0        0    0     0     0   1     0   2   0         0        0   \n",
       "in        0        0    0     0     0   0     2   0   0         0        0   \n",
       "is        1        1    0     0     0   0     0   0   0         2        0   \n",
       "learning  0        0    1     0     2   1     0   0   2         0        2   \n",
       "machine   0        1    1     0     0   0     0   0   0         2        0   \n",
       "of        0        0    0     1     0   0     0   0   0         0        0   \n",
       "part      1        0    0     0     0   0     0   0   0         0        0   \n",
       "science   0        0    1     3     1   0     0   0   1         0        0   \n",
       "useful    0        0    0     1     0   0     0   0   0         0        0   \n",
       "very      0        0    0     0     0   0     0   0   1         0        0   \n",
       "\n",
       "          of  part  science  useful  very  \n",
       "a          0     1        0       0     0  \n",
       "amazing    0     0        0       0     0  \n",
       "and        0     0        1       0     0  \n",
       "data       1     0        3       1     0  \n",
       "deep       0     0        1       0     0  \n",
       "go         0     0        0       0     0  \n",
       "hand       0     0        0       0     0  \n",
       "in         0     0        0       0     0  \n",
       "is         0     0        1       0     1  \n",
       "learning   0     0        0       0     0  \n",
       "machine    0     0        0       0     0  \n",
       "of         0     1        0       0     0  \n",
       "part       1     0        0       0     0  \n",
       "science    0     0        0       0     0  \n",
       "useful     0     0        0       0     1  \n",
       "very       0     0        0       1     0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e1ea8-ab39-4816-8a16-e2b290c4d976",
   "metadata": {},
   "source": [
    "WORD2VEC: WORD EMBEDDING - Used to find similiarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db13bd-74c9-4d4a-8e1e-dc90879ae0d4",
   "metadata": {},
   "source": [
    "## üß† Word2Vec ‚Äî Explanation + Interview Q&A\n",
    "\n",
    "### üîπ What is Word2Vec?\n",
    "Word2Vec is a **word embedding technique** in **Natural Language Processing (NLP)** that converts words into **dense numerical vectors**. These vectors capture **semantic and syntactic relationships** between words.\n",
    "\n",
    "Words with similar meanings have vectors that are close to each other in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Core Idea (Intuition)\n",
    "> Words appearing in similar contexts have similar meanings.\n",
    "\n",
    "Example:\n",
    "king ‚àí man + woman ‚âà queen\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Word2Vec is Needed\n",
    "Traditional methods like Bag of Words and TF-IDF:\n",
    "- Create sparse vectors  \n",
    "- Ignore word meaning and context  \n",
    "\n",
    "Word2Vec solves this by:\n",
    "- Creating dense, low-dimensional vectors  \n",
    "- Capturing semantic similarity  \n",
    "- Preserving relationships between words  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Word2Vec Architectures\n",
    "\n",
    "#### 1Ô∏è‚É£ CBOW (Continuous Bag of Words)\n",
    "- Predicts the **target word** using surrounding context words  \n",
    "- Faster training  \n",
    "- Works well for frequent words  \n",
    "\n",
    "Example:\n",
    "\"I love ___ learning\" ‚Üí machine\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Skip-Gram\n",
    "- Predicts **context words** using the target word  \n",
    "- Better for rare words  \n",
    "- More accurate but slower  \n",
    "\n",
    "Example:\n",
    "machine ‚Üí {I, love, learning}\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Training Objective (Simple)\n",
    "Word2Vec learns word vectors by maximizing the probability of correct word‚Äìcontext pairs and minimizing incorrect ones.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Optimization Techniques\n",
    "- Negative Sampling  \n",
    "- Hierarchical Softmax  \n",
    "\n",
    "These reduce training time for large vocabularies.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Aim of Word2Vec\n",
    "- Learn meaningful word representations  \n",
    "- Capture semantic relationships  \n",
    "- Reduce dimensionality  \n",
    "- Improve NLP model performance  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Why We Use Word2Vec\n",
    "- Understand word similarity  \n",
    "- Improve text classification, search, NLP tasks  \n",
    "- Better than BoW and TF-IDF for meaning  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "- Cannot handle unseen (OOV) words  \n",
    "- Same word has same vector in all contexts  \n",
    "- Requires large datasets  \n",
    "\n",
    "---\n",
    "\n",
    "## üíº Word2Vec ‚Äî Interview Questions and Answers\n",
    "\n",
    "### 1. What is Word2Vec?\n",
    "Word2Vec is a word embedding technique that represents words as dense vectors capturing semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why is Word2Vec better than Bag of Words?\n",
    "Word2Vec captures semantic meaning and context, while Bag of Words only counts word frequency.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What are the two architectures of Word2Vec?\n",
    "- CBOW  \n",
    "- Skip-Gram  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Difference between CBOW and Skip-Gram?\n",
    "- CBOW predicts the target word from context  \n",
    "- Skip-Gram predicts context words from the target  \n",
    "- Skip-Gram works better for rare words  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. What does Word2Vec learn during training?\n",
    "It learns vector representations where semantically similar words are close in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What is Negative Sampling?\n",
    "An optimization technique that updates weights using a small number of negative examples instead of all words.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What is Hierarchical Softmax?\n",
    "An optimization method that speeds up training by using a binary tree structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Is Word2Vec supervised or unsupervised?\n",
    "Word2Vec is **unsupervised**.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. What is the dimensionality of Word2Vec vectors?\n",
    "Usually between **100 to 300**, depending on the application.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Can Word2Vec handle polysemy (multiple meanings)?\n",
    "No. Each word has only one vector representation regardless of context.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. What are limitations of Word2Vec?\n",
    "- Context-independent  \n",
    "- Cannot handle OOV words  \n",
    "- Needs large data  \n",
    "\n",
    "---\n",
    "\n",
    "### 12. Where is Word2Vec used?\n",
    "- Text classification  \n",
    "- Recommendation systems  \n",
    "- Semantic search  \n",
    "- Chatbots  \n",
    "\n",
    "---\n",
    "\n",
    "### 13. Word2Vec vs TF-IDF?\n",
    "Word2Vec captures meaning and similarity, while TF-IDF only measures word importance.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Is Word2Vec still used today?\n",
    "Yes, especially as a foundation for modern embeddings like GloVe, FastText, and contextual models.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. One-line interview answer:\n",
    "> Word2Vec converts words into dense vectors that capture semantic relationships by learning from word contexts.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Summary\n",
    "Word2Vec is a powerful NLP technique that learns meaningful word embeddings using neural networks. It captures word similarity, reduces dimensionality, and significantly improves language understanding compared to traditional text representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d0bef-6a3f-47c1-988f-2b8221a1306f",
   "metadata": {},
   "source": [
    "CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73644088-e008-4578-8e06-04fc701ecc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d1a2755b-b9a5-4950-bbbe-b663b54b88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "41ca8107-c385-41b7-881c-3fd9955ba2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25c72267-fa3f-4e84-94a1-b19252010e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the cat sits on the mat',\n",
    "          'the dog plays in the garden',\n",
    "          'dogs and cats are good pets',\n",
    "          'the garden has beautiful flowers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "64cdb9ca-a5d7-445c-b1b0-1ba2b9890297",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8be2508a-f8f4-4cf0-ba28-b3f0d869cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train word2vec\n",
    "model = Word2Vec(sentences = tokenized_corpus,vector_size=100,window=5,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1279b532-8113-4b72-9ba7-4e55d70da254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00180023  0.00704609  0.0029447  -0.00698085  0.00771268 -0.00598893\n",
      "  0.00899771  0.0029592  -0.00401529 -0.00468899 -0.00441672 -0.00614646\n",
      "  0.00937874 -0.0026496   0.00777244 -0.00968034  0.00210879 -0.00123361\n",
      "  0.00754423 -0.0090546   0.00743756 -0.0051058  -0.00601377 -0.00564916\n",
      " -0.00337917 -0.0034111  -0.00319566 -0.0074922   0.00070878 -0.00057607\n",
      " -0.001684    0.00375713 -0.00762019 -0.00322142  0.00515534  0.00854386\n",
      " -0.00980994  0.00719534  0.00530949 -0.0038797   0.00857616 -0.00922199\n",
      "  0.00724868  0.00536383  0.00129359 -0.00519975 -0.00417865 -0.00335678\n",
      "  0.00160829  0.0015867   0.00738824  0.00997759  0.00886734 -0.00400645\n",
      "  0.00964539 -0.00062954  0.00486543  0.00254902 -0.00062981  0.00366745\n",
      " -0.00531941 -0.00575668 -0.00760464  0.00190643  0.00652587  0.00088213\n",
      "  0.00125695  0.0031716   0.00813467 -0.00770006  0.00226075 -0.00747411\n",
      "  0.00370981  0.00951055  0.00752026  0.00642603  0.00801478  0.00655115\n",
      "  0.00685668  0.00868209 -0.00494804  0.00921295  0.0050592  -0.00213025\n",
      "  0.00848745  0.00508134  0.00964895  0.0028324   0.00986754  0.001197\n",
      "  0.00912918  0.00358697  0.00656481 -0.00361133  0.00679291  0.00724357\n",
      " -0.00213346 -0.00185955  0.00361175 -0.00703643]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "347475fb-a860-48c4-92c8-01f83ff1bf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mat', 0.24666325747966766), ('are', 0.11928337812423706), ('dog', 0.1166219711303711), ('dogs', 0.09614861011505127), ('garden', 0.08545845001935959), ('pets', 0.07172321528196335), ('in', 0.05970005318522453), ('beautiful', 0.04119238629937172), ('cats', 0.012430463917553425), ('flowers', 0.0014541475102305412)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "048c4ab2-d958-4811-bce3-be6c91e371df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('has', 0.25290656089782715), ('garden', 0.13727134466171265), ('cat', 0.1166219711303711), ('pets', 0.04411439225077629), ('mat', 0.02700837142765522), ('good', 0.01281161978840828), ('beautiful', 0.006617163307964802), ('dogs', -0.0011978191323578358), ('are', -0.025455353781580925), ('sits', -0.03247775509953499)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('dog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791db0d-8f91-4645-9447-7932604ff352",
   "metadata": {},
   "source": [
    "DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b1febc1-bcb6-4675-b41d-5171d1358e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5e83c059-9ee4-424c-8b90-8d8b0105b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'machine learning is interdisclplinary field',\n",
    "    'machine learning models improve prediction',\n",
    "    'artificial intelligence is evolving rapidly',\n",
    "    'natural language processing is part of a.i',\n",
    "    'python is widely used for data analytics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4a2bce0-65ea-40a7-ab0f-3aa86dd56127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['machine', 'learning', 'is', 'interdisclplinary', 'field'], tags=['0']),\n",
       " TaggedDocument(words=['machine', 'learning', 'models', 'improve', 'prediction'], tags=['1']),\n",
       " TaggedDocument(words=['artificial', 'intelligence', 'is', 'evolving', 'rapidly'], tags=['2']),\n",
       " TaggedDocument(words=['natural', 'language', 'processing', 'is', 'part', 'of', 'a.i'], tags=['3']),\n",
       " TaggedDocument(words=['python', 'is', 'widely', 'used', 'for', 'data', 'analytics'], tags=['4'])]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()),tags = [str(i)]) for i, doc in enumerate(documents)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ac42e135-5632-4902-8a27-b3e6a30fcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=50,\n",
    "                window=2,\n",
    "                min_count=1,\n",
    "                workers=4,\n",
    "                epochs=100)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9fc99307-096e-4ace-81a1-7360485fa913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Vector: [ 0.0010467  -0.00017597 -0.00212406 -0.00816883  0.00460686  0.00611052\n",
      "  0.00247508 -0.00496717 -0.00378267  0.00199368  0.00675884  0.01034335\n",
      " -0.00233838 -0.00010272 -0.00720694 -0.00812072 -0.00718223  0.00178876\n",
      "  0.00757818 -0.0075379   0.00496635  0.00157846  0.00217059 -0.00760916\n",
      "  0.0034472  -0.00180543  0.00611876 -0.00959643 -0.00835715 -0.00105242\n",
      " -0.00401811  0.01047904  0.00570983 -0.00922242  0.00532508  0.00038369\n",
      " -0.00373882 -0.005643    0.0015627   0.00335722 -0.00682235 -0.00767647\n",
      " -0.00089714 -0.00488439  0.00627467  0.00792897 -0.00356382 -0.00517903\n",
      " -0.0081386   0.0063338 ]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "test_doc = 'machine learning helps in ai advancements'\n",
    "\n",
    "test_vector = model.infer_vector(word_tokenize(test_doc.lower()))\n",
    "\n",
    "print('The Test Vector:', test_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fba69e2f-bb79-42c7-bcd6-595e6be699ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 0.17112192511558533), ('3', 0.13699612021446228)]\n"
     ]
    }
   ],
   "source": [
    "similar_docs = model.dv.most_similar([test_vector],topn=2)\n",
    "print(similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f1e348e4-cb88-4098-8ff1-293167da7b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['machine', 'learning', 'models', 'improve', 'prediction'], tags=['1'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[int(similar_docs[0][0])]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dc9b66d-3da6-4bab-9601-f1e86067251d",
   "metadata": {},
   "source": [
    "PART OF SPEECH(POS)-\n",
    "1.\n",
    "2.\n",
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf130820-5ce7-4248-ba46-e3fac4387f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c33b5dc9-264b-4525-8bc5-513be066cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    return tagged_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e656cce9-d625-4b1e-9a35-3ed85fe4b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The quick brown fox jumps over a lazy dog'\n",
    "pos_tags = pos_tagger(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1ceaf8-88ba-4624-a07b-822da69eb030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('a', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print('POS Tags:',pos_tags)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db7e8be1-8458-41f4-b8df-7b5a88bae904",
   "metadata": {},
   "source": [
    "NAME ENTITY RECOGINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f276c5af-379a-48f8-a901-12c596ebe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2672fd94-c03f-4f4f-8acb-cf10f3ab59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_extraction(text):\n",
    "    doc = nlp(text) \n",
    "    return[(ent.text,ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faa39e23-aff4-4044-bc49-ad0bd3941b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [('Dhirubai Ambani', 'PERSON'), ('Reliance', 'ORG'), ('India', 'GPE'), ('1996', 'DATE'), ('Two', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Dhirubai Ambani is the founder of Reliance in India in 1996. He has Two sons'\n",
    "\n",
    "entities = ner_extraction(sentence)\n",
    "print(\"Named Entities:\",entities)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11e38bce-f9e8-44af-9c6c-7047bf46496e",
   "metadata": {},
   "source": [
    "N-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a892fa0f-c164-43df-9a99-52fa8e85ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "793b26ea-af2f-48bb-8cf6-0e9e0ffdd8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f6ddfe7-1ce7-4177-aa8f-e096a3901573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text,n):\n",
    "    words = word_tokenize(text)\n",
    "    n_grams = list(ngrams(words,n))\n",
    "    return [\" \".join(gram) for gram in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92fa027f-b6c9-4e7b-b9ca-d11bd30b5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram ['I love', 'love Natural', 'Natural Language', 'Language Processing']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love Natural Language Processing\"\n",
    "n = 2\n",
    "bigrams = generate_ngrams(text,n)\n",
    "print(\"Bigram\",bigrams)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77267ebe-9db6-43d6-818b-422cd75c4cb0",
   "metadata": {},
   "source": [
    "PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbb6c119-eec3-4be4-8f50-55c0226d0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(text,n=2):\n",
    "    words = word_tokenize(text.lower())\n",
    "    n_grams = list(ngrams(words,n))\n",
    "    model = Counter(n_grams)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91f9d9c4-702e-47bc-a88d-eb7a9419ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I Love Natural Language Processing.I Love Machine Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "606ed3f9-043e-4672-870e-35d530f18c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram: Counter({('i', 'love'): 1, ('love', 'natural'): 1, ('natural', 'language'): 1, ('language', 'processing.i'): 1, ('processing.i', 'love'): 1, ('love', 'machine'): 1, ('machine', 'learning'): 1})\n"
     ]
    }
   ],
   "source": [
    "bigram_model = train_ngram_model(text,n=2)\n",
    "print(\"bigram:\",bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78ef2179-9e7a-43d0-bcb6-911d6ce515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def predict_next_word(model, input_text, n=2):\n",
    "    words = word_tokenize(input_text.lower())\n",
    "    prev_words = tuple(words[-(n-1):])\n",
    "\n",
    "    candidates = {}\n",
    "\n",
    "    for k, v in model.items():\n",
    "        if k[:-1] == prev_words:\n",
    "            candidates[k] = v\n",
    "\n",
    "    if not candidates:\n",
    "        return \"No Prediction Found\"\n",
    "\n",
    "    next_word = max(candidates, key=candidates.get)[-1]\n",
    "    return next_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "baf1bb83-e0e5-4e83-b092-65505929cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"In modern data science workflows, machine learning models are trained on large and diverse datasets to extract meaningful insights, reduce uncertainty, improve decision making, and support intelligent systems used across industries such as healthcare, finance, marketing, and artificial intelligence research.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9970566-df9e-4338-afaa-93b423e3ffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Next Word: No Prediction Found\n"
     ]
    }
   ],
   "source": [
    "predicted_word = predict_next_word(bigram_model,input_text,n=2)\n",
    "print(\"Predicted Next Word:\",predicted_word)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2020e84a-c57f-460a-b912-6ca5e21564ec",
   "metadata": {},
   "source": [
    "SPACY LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ca5b67-102e-46af-a381-6c8b97cdc1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dhirubai Ambani is the founder of Reliance in India in 1996. He has two sons.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Dhirubai Ambani is the founder of Reliance in India in 1996. He has two sons.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe43543d-5570-42d3-8385-b49fc03a496a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d04bd3-1cb1-41a4-b42e-378c7dba0f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toekn:\n",
      "Dhirubai\n",
      "Ambani\n",
      "is\n",
      "the\n",
      "founder\n",
      "of\n",
      "Reliance\n",
      "in\n",
      "India\n",
      "in\n",
      "1996\n",
      ".\n",
      "He\n",
      "has\n",
      "two\n",
      "sons\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "print(\"toekn:\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404e9414-e117-4fb3-9fb0-7db737b00521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tags:\n",
      "Dhirubai : PROPN (NNP)\n",
      "Ambani : PROPN (NNP)\n",
      "is : AUX (VBZ)\n",
      "the : DET (DT)\n",
      "founder : NOUN (NN)\n",
      "of : ADP (IN)\n",
      "Reliance : PROPN (NNP)\n",
      "in : ADP (IN)\n",
      "India : PROPN (NNP)\n",
      "in : ADP (IN)\n",
      "1996 : NUM (CD)\n",
      ". : PUNCT (.)\n",
      "He : PRON (PRP)\n",
      "has : VERB (VBZ)\n",
      "two : NUM (CD)\n",
      "sons : NOUN (NNS)\n",
      ". : PUNCT (.)\n"
     ]
    }
   ],
   "source": [
    "# Part of Speech\n",
    "print('\\nPOS Tags:')\n",
    "for token in doc:\n",
    "    print(f\"{token.text} : {token.pos_} ({token.tag_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9aec163-d338-4b47-800b-30f7c4798706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmas:\n",
      "Dhirubai: Dhirubai\n",
      "Ambani: Ambani\n",
      "is: be\n",
      "the: the\n",
      "founder: founder\n",
      "of: of\n",
      "Reliance: Reliance\n",
      "in: in\n",
      "India: India\n",
      "in: in\n",
      "1996: 1996\n",
      ".: .\n",
      "He: he\n",
      "has: have\n",
      "two: two\n",
      "sons: son\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "print(\"\\nLemmas:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e944e162-29ae-4f85-80cd-58fe86572e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities:\n",
      "Dhirubai Ambani:PERSON\n",
      "Reliance:ORG\n",
      "India:GPE\n",
      "1996:DATE\n",
      "two:CARDINAL\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recoginition\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}:{ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b93f234-93f1-4fbe-9643-5861df2c6ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency Parsing:\n",
      "Dhirubai: compound,head:Ambani\n",
      "Ambani: nsubj,head:is\n",
      "is: ROOT,head:is\n",
      "the: det,head:founder\n",
      "founder: attr,head:is\n",
      "of: prep,head:founder\n",
      "Reliance: pobj,head:of\n",
      "in: prep,head:Reliance\n",
      "India: pobj,head:in\n",
      "in: prep,head:founder\n",
      "1996: pobj,head:in\n",
      ".: punct,head:is\n",
      "He: nsubj,head:has\n",
      "has: ROOT,head:has\n",
      "two: nummod,head:sons\n",
      "sons: dobj,head:has\n",
      ".: punct,head:has\n"
     ]
    }
   ],
   "source": [
    "#Dependency Parsing\n",
    "print(\"\\nDependency Parsing:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.dep_},head:{token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9768a84f-7889-407a-802f-597d2e8bcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences:\n",
      "Dhirubai Ambani is the founder of Reliance in India in 1996.\n",
      "He has two sons.\n"
     ]
    }
   ],
   "source": [
    "#Sentence Segmentation\n",
    "print(\"\\nSentences:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31329959-8fe0-4399-a44f-b27524409e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Vectors and Similarity:\n",
      "Similarity between king and queen: 0.725\n",
      "Similarity between king and apple: 0.235\n"
     ]
    }
   ],
   "source": [
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "doc_lg = nlp_lg(\"king queen apple\")\n",
    "\n",
    "print(\"\\nWord Vectors and Similarity:\")\n",
    "\n",
    "king = doc_lg[0]\n",
    "queen = doc_lg[1]\n",
    "apple = doc_lg[2]\n",
    "\n",
    "print(f\"Similarity between king and queen: {king.similarity(queen):.3f}\")\n",
    "print(f\"Similarity between king and apple: {king.similarity(apple):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59d45be3-6fc6-4a2f-9598-46fb0a879fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Apple Inc, Pattern ID: AppleIncPattern\n",
      "Matched span: located in, Pattern ID: LocatedInPattern\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define patterns\n",
    "pattern = [{'LOWER': 'apple'}, {'LOWER': 'inc'}]\n",
    "pattern1 = [{'LOWER': 'located'}, {'LOWER': 'in'}]\n",
    "\n",
    "# Add patterns to matcher\n",
    "matcher.add(\"AppleIncPattern\", [pattern])\n",
    "matcher.add(\"LocatedInPattern\", [pattern1])\n",
    "\n",
    "# Create doc\n",
    "doc = nlp(\"Apple Inc is located in India.\")\n",
    "\n",
    "# Apply matcher\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print matches\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(f\"Matched span: {span.text}, Pattern ID: {string_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1c45b-738e-4f24-93ac-200798676186",
   "metadata": {},
   "source": [
    "## üß† TextBlob ‚Äî Explanation + Interview Q&A\n",
    "\n",
    "### üîπ What is TextBlob?\n",
    "TextBlob is a **Python library for Natural Language Processing (NLP)** that provides a **simple API** for performing common NLP tasks such as text preprocessing, sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and spelling correction.\n",
    "\n",
    "TextBlob is built on top of **NLTK** and **Pattern**, making it beginner-friendly and easy to use.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Do We Use TextBlob?\n",
    "TextBlob is mainly used for:\n",
    "- Quick NLP prototyping  \n",
    "- Simple text analysis tasks  \n",
    "- Educational and beginner-level NLP projects  \n",
    "\n",
    "It hides complex NLP operations behind an easy-to-use interface.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Core NLP Tasks Supported by TextBlob\n",
    "\n",
    "- Tokenization  \n",
    "- Part-of-Speech (POS) tagging  \n",
    "- Noun phrase extraction  \n",
    "- Sentiment analysis  \n",
    "- Lemmatization  \n",
    "- Spelling correction  \n",
    "- Translation (via APIs)  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Sentiment Analysis in TextBlob\n",
    "\n",
    "TextBlob provides **rule-based sentiment analysis**.\n",
    "\n",
    "It returns two values:\n",
    "- **Polarity** ‚Üí range [-1, +1]  \n",
    "  - Negative ‚Üí negative sentiment  \n",
    "  - Positive ‚Üí positive sentiment  \n",
    "- **Subjectivity** ‚Üí range [0, 1]  \n",
    "  - 0 ‚Üí factual  \n",
    "  - 1 ‚Üí opinion-based  \n",
    "\n",
    "Example:\n",
    "- Polarity = 0.8 ‚Üí Positive sentiment  \n",
    "- Subjectivity = 0.9 ‚Üí Highly subjective  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ How TextBlob Works (Internally)\n",
    "- Uses predefined lexicons and rules  \n",
    "- Relies on Pattern library for sentiment  \n",
    "- Does not require model training  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Aim of TextBlob\n",
    "\n",
    "The main aims are to:\n",
    "- Simplify NLP tasks  \n",
    "- Enable fast text analysis  \n",
    "- Provide readable and easy NLP outputs  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Why TextBlob is Popular\n",
    "- Very easy to use  \n",
    "- Minimal code required  \n",
    "- Great for quick experiments  \n",
    "- Good for sentiment analysis demos  \n",
    "\n",
    "---\n",
    "\n",
    "### üìä Advantages\n",
    "\n",
    "- Beginner-friendly  \n",
    "- Clean and intuitive API  \n",
    "- No training required  \n",
    "- Works out-of-the-box  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "- Not suitable for large-scale NLP  \n",
    "- Rule-based sentiment (less accurate)  \n",
    "- Limited customization  \n",
    "- Slower for big datasets  \n",
    "\n",
    "---\n",
    "\n",
    "## üíº TextBlob ‚Äî Interview Questions and Answers\n",
    "\n",
    "### 1. What is TextBlob?\n",
    "TextBlob is a Python NLP library used for simple text processing and sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Is TextBlob rule-based or model-based?\n",
    "TextBlob is **rule-based**, not machine-learning based.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Which libraries does TextBlob use internally?\n",
    "TextBlob is built on top of **NLTK** and **Pattern**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What sentiment metrics does TextBlob provide?\n",
    "- Polarity  \n",
    "- Subjectivity  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. What is polarity in TextBlob?\n",
    "Polarity measures sentiment from -1 (negative) to +1 (positive).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What is subjectivity in TextBlob?\n",
    "Subjectivity measures how opinionated a text is, ranging from 0 to 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Can TextBlob be used for machine learning?\n",
    "No. TextBlob itself is not a machine learning model.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Does TextBlob require training?\n",
    "No. It works directly using predefined rules and lexicons.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. TextBlob vs NLTK?\n",
    "- TextBlob is simpler  \n",
    "- NLTK is more flexible and powerful  \n",
    "\n",
    "---\n",
    "\n",
    "### 10. TextBlob vs spaCy?\n",
    "- TextBlob is easier but limited  \n",
    "- spaCy is faster and production-ready  \n",
    "\n",
    "---\n",
    "\n",
    "### 11. Can TextBlob perform POS tagging?\n",
    "Yes. TextBlob supports Part-of-Speech tagging.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Is TextBlob suitable for production systems?\n",
    "No. It is best for prototyping and learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. What are common use cases of TextBlob?\n",
    "- Sentiment analysis  \n",
    "- Text preprocessing  \n",
    "- Educational NLP projects  \n",
    "\n",
    "---\n",
    "\n",
    "### 14. Does TextBlob support lemmatization?\n",
    "Yes. It supports basic lemmatization.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. One-line interview answer:\n",
    "> TextBlob is a Python NLP library that simplifies common text processing tasks using an easy-to-use, rule-based approach.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Summary\n",
    "TextBlob is a lightweight NLP library designed for simplicity and ease of use. It is ideal for beginners, quick sentiment analysis, and rapid NLP prototyping, but not recommended for large-scale or production-grade systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bef2457-3dba-45e0-9dc5-bb789db26909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['I', 'love', 'learning', 'data', 'science', 'It', 'is', 'very', 'interesting', 'and', 'useful']\n",
      "Sentences: [Sentence(\"I love learning data science.\"), Sentence(\"It is very interesting and useful.\")]\n",
      "Polarity: 0.48333333333333334\n",
      "Subjectivity: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love learning data science. It is very interesting and useful.\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Tokenization\n",
    "print(\"Words:\", blob.words)\n",
    "print(\"Sentences:\", blob.sentences)\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1af27feb-9c1f-4c47-a873-5aab8a0e445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 8\n",
      "Overall sentiment: Sentiment(polarity=0.05738095238095239, subjectivity=0.4172619047619047)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"\"\"\n",
    "Over the past few years, data science has become one of the most in-demand career paths across industries.\n",
    "Many students start their journey with excitement, but soon realize that learning data science is not just\n",
    "about writing Python code or building machine learning models. It requires strong foundations in statistics,\n",
    "a clear understanding of data cleaning, and the patience to analyze large, messy datasets.\n",
    "\n",
    "During the learning process, beginners often feel overwhelmed by complex algorithms and unfamiliar\n",
    "mathematical concepts. Sometimes the results of a model are disappointing, even after hours of effort,\n",
    "which can be frustrating and discouraging. However, with consistent practice, real-world projects, and\n",
    "continuous learning, confidence slowly builds.\n",
    "\n",
    "Despite the challenges, the satisfaction of extracting meaningful insights from data, improving business\n",
    "decisions, and solving real problems makes the journey worthwhile. For those who remain persistent, data\n",
    "science offers not only professional growth but also a sense of achievement and long-term career stability.\n",
    "\"\"\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(\"Number of sentences:\", len(blob.sentences))\n",
    "print(\"Overall sentiment:\", blob.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4cfe5-7206-4f1e-a5ce-c9fc36da69cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "817f65de-763d-48b3-b97d-cedcd84823e4",
   "metadata": {},
   "source": [
    "## üìä TF-IDF (Term Frequency ‚Äì Inverse Document Frequency) ‚Äî Detailed Explanation\n",
    "\n",
    "TF-IDF is a **text vectorization technique** used in **Natural Language Processing (NLP)** to convert text into numerical form. It measures **how important a word is to a document relative to a collection of documents (corpus)**.\n",
    "\n",
    "Unlike Bag of Words, TF-IDF reduces the importance of **common words** and increases the importance of **rare but meaningful words**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why TF-IDF is Needed\n",
    "\n",
    "In Bag of Words:\n",
    "- Common words like *is, the, and* get high importance\n",
    "- Rare but informative words are ignored\n",
    "\n",
    "TF-IDF solves this by:\n",
    "- Down-weighting frequent words\n",
    "- Up-weighting rare, informative words\n",
    "- Improving text classification performance\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Components of TF-IDF\n",
    "\n",
    "TF-IDF is a product of **two values**:\n",
    "\n",
    "TF-IDF = TF √ó IDF\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Term Frequency (TF)\n",
    "\n",
    "Term Frequency measures **how often a word appears in a document**.\n",
    "\n",
    "### Formula:\n",
    "TF(t, d) = (Number of times term *t* appears in document *d*)  \n",
    "             / (Total number of terms in document *d*)\n",
    "\n",
    "### Intuition:\n",
    "- Higher frequency ‚Üí more important within that document\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency measures **how rare a word is across all documents**.\n",
    "\n",
    "### Formula:\n",
    "IDF(t) = log( N / (1 + df(t)) )\n",
    "\n",
    "where:\n",
    "- N = total number of documents  \n",
    "- df(t) = number of documents containing term *t*  \n",
    "- 1 is added to avoid division by zero\n",
    "\n",
    "### Intuition:\n",
    "- Rare word ‚Üí high IDF  \n",
    "- Common word ‚Üí low IDF  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Final TF-IDF Formula\n",
    "\n",
    "TF-IDF(t, d) = TF(t, d) √ó IDF(t)\n",
    "\n",
    "This score reflects **both local importance (TF)** and **global importance (IDF)**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example (Simple)\n",
    "\n",
    "Documents:\n",
    "- Doc1: \"machine learning is powerful\"\n",
    "- Doc2: \"machine learning is popular\"\n",
    "\n",
    "Word **\"machine\"**:\n",
    "- Appears in both documents ‚Üí low IDF  \n",
    "- TF-IDF score is low\n",
    "\n",
    "Word **\"powerful\"**:\n",
    "- Appears only in Doc1 ‚Üí high IDF  \n",
    "- TF-IDF score is high\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How TF-IDF Works (Step-by-Step)\n",
    "\n",
    "1. Preprocess text (lowercase, stopword removal, tokenization)\n",
    "2. Calculate TF for each word in each document\n",
    "3. Calculate IDF for each word across corpus\n",
    "4. Multiply TF and IDF\n",
    "5. Convert documents into TF-IDF vectors\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Aim of TF-IDF\n",
    "\n",
    "The main aims are to:\n",
    "- Identify important words in documents\n",
    "- Reduce impact of common words\n",
    "- Improve text classification and search results\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Why We Use TF-IDF\n",
    "\n",
    "TF-IDF is used because it:\n",
    "- Is simple and effective\n",
    "- Improves over Bag of Words\n",
    "- Works well with classical ML models\n",
    "- Requires no training data\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Variants of TF-IDF\n",
    "\n",
    "- **Binary TF-IDF** ‚Äì presence/absence\n",
    "- **Sublinear TF** ‚Äì uses log(TF)\n",
    "- **N-gram TF-IDF** ‚Äì captures word sequences\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Advantages\n",
    "\n",
    "- Easy to understand and implement\n",
    "- Highlights informative words\n",
    "- Reduces noise from frequent words\n",
    "- Efficient for large text corpora\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limitations\n",
    "\n",
    "- Ignores word order\n",
    "- Cannot capture semantics or context\n",
    "- High dimensional sparse vectors\n",
    "- Same word has same meaning everywhere\n",
    "\n",
    "---\n",
    "\n",
    "## üß† TF-IDF vs Bag of Words (Quick)\n",
    "\n",
    "| Feature | TF-IDF | Bag of Words |\n",
    "|------|-------|--------------|\n",
    "| Importance | Weighted | Raw count |\n",
    "| Common words | Down-weighted | Overweighted |\n",
    "| Meaning | Partial | None |\n",
    "| Sparsity | High | High |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† In Simple Words\n",
    "\n",
    "TF-IDF tells us **which words matter most** in a document by balancing how often they appear with how rare they are across all documents.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Summary\n",
    "\n",
    "TF-IDF is a powerful and widely used text representation technique that improves upon Bag of Words by emphasizing meaningful words and suppressing common ones. It is a strong baseline for many NLP tasks such as text classification, search engines, and information retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6fcde-1150-4741-8ce1-d6fb8f6bd8bf",
   "metadata": {},
   "source": [
    "## üìä TF-IDF ‚Äî Interview Questions and Answers\n",
    "\n",
    "### 1. What is TF-IDF?\n",
    "TF-IDF (Term Frequency‚ÄìInverse Document Frequency) is a text vectorization technique that measures how important a word is in a document relative to a corpus.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why is TF-IDF used instead of Bag of Words?\n",
    "TF-IDF reduces the weight of common words and increases the importance of rare but meaningful words, unlike Bag of Words which only counts frequency.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What are the two components of TF-IDF?\n",
    "- Term Frequency (TF)  \n",
    "- Inverse Document Frequency (IDF)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What is Term Frequency (TF)?\n",
    "TF measures how often a term appears in a document.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What is Inverse Document Frequency (IDF)?\n",
    "IDF measures how rare a term is across all documents.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Write the TF-IDF formula.\n",
    "TF-IDF(t, d) = TF(t, d) √ó IDF(t)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What happens if a word appears in every document?\n",
    "Its IDF becomes very low, so its TF-IDF score is low.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What happens if a word appears in only one document?\n",
    "Its IDF is high, making its TF-IDF score high.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Does TF-IDF consider word order?\n",
    "No. TF-IDF ignores word order.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Does TF-IDF capture semantic meaning?\n",
    "No. It only measures statistical importance.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. What type of vector does TF-IDF produce?\n",
    "Sparse, high-dimensional numerical vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Is TF-IDF supervised or unsupervised?\n",
    "TF-IDF is an unsupervised feature extraction technique.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. What preprocessing steps are required for TF-IDF?\n",
    "- Lowercasing  \n",
    "- Tokenization  \n",
    "- Stopword removal  \n",
    "- Stemming or lemmatization  \n",
    "\n",
    "---\n",
    "\n",
    "### 14. Can TF-IDF handle new unseen words?\n",
    "No. Words not in the vocabulary are ignored.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. What are advantages of TF-IDF?\n",
    "- Simple and effective  \n",
    "- Reduces importance of common words  \n",
    "- Improves text classification  \n",
    "\n",
    "---\n",
    "\n",
    "### 16. What are limitations of TF-IDF?\n",
    "- Ignores context and meaning  \n",
    "- High dimensionality  \n",
    "- Sparse representation  \n",
    "\n",
    "---\n",
    "\n",
    "### 17. TF-IDF vs Word2Vec?\n",
    "TF-IDF is frequency-based, while Word2Vec captures semantic relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### 18. TF-IDF vs Count Vectorizer?\n",
    "TF-IDF weights terms by importance, Count Vectorizer uses raw counts.\n",
    "\n",
    "---\n",
    "\n",
    "### 19. Where is TF-IDF commonly used?\n",
    "- Text classification  \n",
    "- Search engines  \n",
    "- Information retrieval  \n",
    "- Spam detection  \n",
    "\n",
    "---\n",
    "\n",
    "### 20. What is sublinear TF scaling?\n",
    "It uses log(TF) instead of raw frequency to reduce the effect of very frequent words.\n",
    "\n",
    "---\n",
    "\n",
    "### 21. What is smoothing in IDF?\n",
    "Adding 1 to denominator to avoid division by zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 22. Does TF-IDF work well with deep learning?\n",
    "No. Deep learning prefers dense embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### 23. Can TF-IDF be used for large datasets?\n",
    "Yes, but memory usage may increase due to high dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "### 24. Is TF-IDF still relevant today?\n",
    "Yes. It is a strong baseline and widely used in classical NLP.\n",
    "\n",
    "---\n",
    "\n",
    "### 25. One-line interview answer:\n",
    "> TF-IDF measures word importance by combining how frequently a word appears in a document with how rare it is across documents.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57261010-2f5a-4c09-986c-c0e90958545c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22183280-8b07-44f0-8c9e-e00f752d093b",
   "metadata": {},
   "source": [
    "## ü§ñ BERT (Bidirectional Encoder Representations from Transformers) ‚Äî Detailed Explanation\n",
    "\n",
    "BERT is a **pre-trained deep learning language model** developed by Google for **Natural Language Processing (NLP)** tasks. Unlike traditional models that read text in one direction, BERT reads text **bidirectionally**, meaning it understands a word based on **both its left and right context**.\n",
    "\n",
    "This bidirectional understanding allows BERT to capture **deep semantic meaning and context**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why BERT Was Needed\n",
    "\n",
    "Earlier NLP models (BoW, TF-IDF, Word2Vec):\n",
    "- Could not fully understand context  \n",
    "- Were often unidirectional  \n",
    "- Treated the same word the same in all sentences  \n",
    "\n",
    "BERT solves this by:\n",
    "- Understanding context from both directions  \n",
    "- Producing **context-aware word representations**  \n",
    "- Improving performance across many NLP tasks  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Core Idea of BERT (Simple Intuition)\n",
    "\n",
    "> The meaning of a word depends on the words around it ‚Äî on both sides.\n",
    "\n",
    "Example:\n",
    "- \"bank\" in *river bank* vs *bank account*\n",
    "\n",
    "BERT assigns **different embeddings** to the same word depending on context.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What Does ‚ÄúBidirectional‚Äù Mean?\n",
    "\n",
    "Traditional models:\n",
    "- Read text left ‚Üí right or right ‚Üí left  \n",
    "\n",
    "BERT:\n",
    "- Reads the entire sentence **at once**\n",
    "- Uses both previous and next words to understand meaning  \n",
    "\n",
    "This is called **deep bidirectional context**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ BERT Architecture (High Level)\n",
    "\n",
    "BERT is based on the **Transformer Encoder** architecture.\n",
    "\n",
    "Key components:\n",
    "- Multi-Head Self-Attention  \n",
    "- Positional Embeddings  \n",
    "- Feed-Forward Neural Networks  \n",
    "- Layer Normalization  \n",
    "\n",
    "BERT uses **only encoders**, not decoders.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Pretraining Tasks in BERT\n",
    "\n",
    "BERT is trained using two self-supervised tasks:\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Masked Language Modeling (MLM)\n",
    "\n",
    "- Randomly masks some words in a sentence  \n",
    "- Model predicts the masked words using context  \n",
    "\n",
    "Example:\n",
    "\"I love [MASK] learning\" ‚Üí machine\n",
    "\n",
    "This forces BERT to learn bidirectional context.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Next Sentence Prediction (NSP)\n",
    "\n",
    "- Model learns whether one sentence logically follows another  \n",
    "\n",
    "This helps in tasks like Question Answering and Natural Language Inference.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Fine-Tuning BERT\n",
    "\n",
    "After pretraining, BERT is **fine-tuned** for specific tasks by adding a small task-specific layer.\n",
    "\n",
    "Common tasks:\n",
    "- Text classification  \n",
    "- Sentiment analysis  \n",
    "- Named Entity Recognition (NER)  \n",
    "- Question Answering  \n",
    "\n",
    "Fine-tuning requires much less data compared to training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Aim of BERT\n",
    "\n",
    "The main aims are to:\n",
    "- Understand language context deeply  \n",
    "- Provide a universal language model  \n",
    "- Improve performance across NLP tasks  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Why We Use BERT\n",
    "\n",
    "BERT is used because it:\n",
    "- Captures deep contextual meaning  \n",
    "- Produces state-of-the-art results  \n",
    "- Works well across many NLP tasks  \n",
    "- Reduces need for task-specific architectures  \n",
    "\n",
    "---\n",
    "\n",
    "## üìä Advantages\n",
    "\n",
    "- Context-aware embeddings  \n",
    "- Bidirectional understanding  \n",
    "- Pretrained on massive text corpora  \n",
    "- Easy fine-tuning  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limitations\n",
    "\n",
    "- Computationally expensive  \n",
    "- Large memory requirement  \n",
    "- Slower inference  \n",
    "- Not ideal for real-time systems  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† BERT vs Word2Vec (Quick)\n",
    "\n",
    "| Feature | BERT | Word2Vec |\n",
    "|------|------|---------|\n",
    "| Context-aware | Yes | No |\n",
    "| Direction | Bidirectional | Context window |\n",
    "| Embedding type | Dynamic | Static |\n",
    "| Accuracy | Very high | Moderate |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† In Simple Words\n",
    "\n",
    "BERT understands language by reading sentences from both sides at the same time. This allows it to understand meaning, context, and relationships far better than older NLP models.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Summary\n",
    "\n",
    "BERT is a transformer-based, bidirectional language model that revolutionized NLP by introducing deep contextual understanding. It serves as the foundation for many modern NLP systems and models like RoBERTa, ALBERT, and DistilBERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6d5fb-e657-4f78-96a5-466bbe7f193e",
   "metadata": {},
   "source": [
    "## üß† Important NLP Topics (Must-Know)\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Tokenization (Word, Subword, Sentence)\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units called **tokens**.\n",
    "\n",
    "Types:\n",
    "- Word Tokenization  \n",
    "- Sentence Tokenization  \n",
    "- Subword Tokenization (BPE, WordPiece)\n",
    "\n",
    "Why important:\n",
    "- Foundation of all NLP pipelines  \n",
    "- Used in models like BERT (WordPiece)\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Stopwords Removal\n",
    "\n",
    "Stopwords are common words like:\n",
    "- is, the, and, a, in\n",
    "\n",
    "Why used:\n",
    "- Reduce noise  \n",
    "- Improve model efficiency  \n",
    "\n",
    "Caution:\n",
    "- Removing stopwords may harm meaning in some tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ N-Grams\n",
    "\n",
    "N-Grams are **contiguous sequences of N words**.\n",
    "\n",
    "Examples:\n",
    "- Unigram ‚Üí \"machine\"  \n",
    "- Bigram ‚Üí \"machine learning\"  \n",
    "- Trigram ‚Üí \"deep learning model\"\n",
    "\n",
    "Why important:\n",
    "- Capture local context  \n",
    "- Used in BoW and TF-IDF\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Named Entity Recognition (NER)\n",
    "\n",
    "NER identifies real-world entities in text.\n",
    "\n",
    "Examples:\n",
    "- Person ‚Üí Elon Musk  \n",
    "- Location ‚Üí India  \n",
    "- Organization ‚Üí Google  \n",
    "\n",
    "Use cases:\n",
    "- Information extraction  \n",
    "- Chatbots  \n",
    "- Search engines\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging assigns grammatical roles to words.\n",
    "\n",
    "Examples:\n",
    "- Noun, Verb, Adjective  \n",
    "\n",
    "Why important:\n",
    "- Improves parsing  \n",
    "- Used in lemmatization and syntax analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Topic Modeling\n",
    "\n",
    "Topic modeling discovers **hidden topics** in documents.\n",
    "\n",
    "Popular methods:\n",
    "- LDA (Latent Dirichlet Allocation)\n",
    "- NMF\n",
    "\n",
    "Use cases:\n",
    "- Document clustering  \n",
    "- News categorization\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Language Models (LM)\n",
    "\n",
    "Language models predict the **next word** in a sequence.\n",
    "\n",
    "Examples:\n",
    "- N-gram LM  \n",
    "- GPT, BERT (Transformer-based)\n",
    "\n",
    "Why important:\n",
    "- Foundation of modern NLP\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Transformers (Core Concept)\n",
    "\n",
    "Transformers use **self-attention** instead of recurrence.\n",
    "\n",
    "Key ideas:\n",
    "- Self-Attention  \n",
    "- Multi-Head Attention  \n",
    "- Positional Encoding  \n",
    "\n",
    "Why important:\n",
    "- Backbone of BERT, GPT, T5\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Attention Mechanism\n",
    "\n",
    "Attention allows the model to **focus on relevant words**.\n",
    "\n",
    "Why important:\n",
    "- Solves long-distance dependency problem  \n",
    "- Improves translation and QA\n",
    "\n",
    "---\n",
    "\n",
    "## üîü Text Classification Pipeline (End-to-End)\n",
    "\n",
    "Steps:\n",
    "1. Text preprocessing  \n",
    "2. Vectorization (TF-IDF / Embeddings)  \n",
    "3. Model training  \n",
    "4. Evaluation  \n",
    "\n",
    "Frequently asked in interviews.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£1Ô∏è‚É£ Evaluation Metrics in NLP\n",
    "\n",
    "- Accuracy  \n",
    "- Precision, Recall, F1-score  \n",
    "- BLEU (Translation)  \n",
    "- ROUGE (Summarization)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£2Ô∏è‚É£ NLP Challenges (Very Important)\n",
    "\n",
    "- Ambiguity  \n",
    "- Sarcasm  \n",
    "- Context understanding  \n",
    "- Multilingual text  \n",
    "- Out-of-vocabulary words  \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£3Ô∏è‚É£ Classical vs Modern NLP\n",
    "\n",
    "| Classical NLP | Modern NLP |\n",
    "|-------------|------------|\n",
    "| TF-IDF | Word Embeddings |\n",
    "| Naive Bayes | BERT |\n",
    "| Rule-based | Transformer-based |\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£4Ô∏è‚É£ NLP Applications (Interview Favorite)\n",
    "\n",
    "- Chatbots  \n",
    "- Sentiment Analysis  \n",
    "- Spam Detection  \n",
    "- Search Engines  \n",
    "- Machine Translation  \n",
    "- Text Summarization  \n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Final Tip for Interviews\n",
    "\n",
    "If you know these **5 things well**, you are interview-ready:\n",
    "1. Text preprocessing  \n",
    "2. TF-IDF vs Word Embeddings  \n",
    "3. Word2Vec vs BERT  \n",
    "4. Text classification pipeline  \n",
    "5. NLP evaluation metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Summary\n",
    "\n",
    "These topics complete the **full NLP interview and project-ready syllabus**. Mastering them along with BERT, TF-IDF, Word2Vec, TextBlob, and preprocessing techniques makes you **strongly prepared for NLP roles**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5af92f-60fc-4bd6-ac7a-d887552d8a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ab2e8-f9f7-4f80-b994-76c1e054ad82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce82ce-0998-48e4-aff9-4b36f1ba4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
